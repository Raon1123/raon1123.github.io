---
title: 인공지능의 역사
tags: [AI, Study]
---

개인적인 취향에 맞추어 인공지능 발전의 역사에서 중요하다고 생각되는 논문을 추린 목록입니다.
되도록이면 시간의 순서에 따라 적고자 하였으나, 비슷한 주제가 있으면 묶어서 설명하고자 하였습니다.


# 개관

- [Wikipedia: History of AI](https://en.wikipedia.org/wiki/History_of_artificial_intelligence)
- [Deep Learning in Neural Networks: An Overview](https://arxiv.org/abs/1404.7828)

# 인공지능의 출발
- A. M. Turing, [COMPUTING MACHINERY AND INTELLIGENCE](https://academic.oup.com/mind/article/LIX/236/433/986238)
- F. Rosenblatt, [The perceptron: A probabilistic model for information storage and organization in the brain.](https://psycnet.apa.org/record/1959-09865-001), Psychological Review, 65(6), 386–408. https://doi.org/10.1037/h0042519
- J. McCarthy et al. [A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence](http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html)

# Support Vector Machine

- SVM
- Kernel Methods

# Learning Theory

- PAC Learning
- VC Dimension

# Neural Networks

- Backpropagation
- Universal Approximation Theorem
- "*[Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/document/726791)*": about convolutional neural networks

# Datasets

- Yann LeCun et al. [MNIST](http://yann.lecun.com/exdb/mnist/)
- "*[ImageNet: A large-scale hierarchical image database](https://www.image-net.org/)*"
- "*[Microsoft COCO: Common Objects in Context](https://arxiv.org/abs/1405.0312)*"

# ImageNet Challenge

- "*[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)*"
- "*[Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)*"
- "*[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)*"
- "*[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)*"
- "*[Rethinking ImageNet Pre-training](https://arxiv.org/abs/1811.08883)*"

# Learning Methods

## Optimization

- "*[On Optimization Methods for Deep Learning](https://icml.cc/2011/papers/210_icmlpaper.pdf)*"
- "*[Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a.html)*"
- "*[Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)*"
- "*[On Network Design Spaces for Visual Recognition](https://arxiv.org/abs/1905.13214)*"

## Generalization (include Regularization)

- "*[Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/10.5555/2627435.2670313)*"
- "*[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)*"
- "*[Layer Normalization](https://arxiv.org/abs/1607.06450)*"
- "*[Group Normalization](https://arxiv.org/abs/1803.08494)*"

# Recurrent Neural Networks

- LSTM

# Reinforcement Learning

- Q-Learning
- Alpha Go

# Transformers

- Ashish Vaswani et al. [Transformers](https://arxiv.org/abs/1706.03762), 2017
- Alexey Dosovitskiy et al. [Vision Transformers(ViT)](https://arxiv.org/abs/2010.11929), 2020

# Large-Language Models

- GPT
- LLaMA

# Generation Models (From Bayesian To Diffusion)

- Bayesian
- Flow based model
- Energy based model
- GAN
- Diffusion model